{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spooky Author Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:23:05.268392Z",
     "start_time": "2019-02-03T09:23:01.826955Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  \n",
    "\n",
    "# set options for rendering plots\n",
    "%matplotlib inline\n",
    "\n",
    "# display multiple outputs within a cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\";\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:23:10.335769Z",
     "start_time": "2019-02-03T09:23:10.190973Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:/Users/1394852/Desktop/ml_project/spooky_author/train.csv\")\n",
    "test = pd.read_csv(\"C:/Users/1394852/Desktop/ml_project/spooky_author/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop words and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:23:12.950002Z",
     "start_time": "2019-02-03T09:23:12.536429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'never', 'once', 'occurred', 'to', 'me', 'that', 'the', 'fumbling', 'might', 'be', 'a', 'mere', 'mistake', '.']\n",
      "['It', 'never', 'occurred', 'fumbling', 'might', 'mere', 'mistake', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "data = train[\"text\"]\n",
    "  \n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "word_tokens = word_tokenize(data[1]) \n",
    "  \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "print(word_tokens) \n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:23:16.152182Z",
     "start_time": "2019-02-03T09:23:14.558509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'process', 'however', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the', 'dimensions', 'of', 'my', 'dungeon', 'as', 'might', 'make', 'its', 'circuit', 'and', 'return', 'to', 'the', 'point', 'whence', 'set', 'out', 'without', 'being', 'aware', 'of', 'the', 'fact', 'so', 'perfectly', 'uniform', 'seemed', 'the', 'wall']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:23:18.178861Z",
     "start_time": "2019-02-03T09:23:18.109861Z"
    }
   },
   "outputs": [],
   "source": [
    "# flatten list and join together as a string\n",
    "flat_list = [item for sublist in data_words for item in sublist]\n",
    "str1 = ' '.join(flat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bigram, trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:23:29.505174Z",
     "start_time": "2019-02-03T09:23:20.733692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'process', 'however', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the', 'dimensions', 'of', 'my', 'dungeon', 'as', 'might', 'make', 'its', 'circuit', 'and', 'return', 'to', 'the', 'point', 'whence', 'set', 'out', 'without', 'being', 'aware', 'of', 'the', 'fact', 'so', 'perfectly', 'uniform', 'seemed', 'the', 'wall']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:23:35.470923Z",
     "start_time": "2019-02-03T09:23:35.442923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'herbert west', 1073.8432406519655),\n",
       " (b'von kempelen', 2514.070707070707),\n",
       " (b'cut off', 178.57336134884434),\n",
       " (b'old bugs', 183.65776269185358),\n",
       " (b'few moments', 112.51943942133815),\n",
       " (b'no longer', 104.39493879326415),\n",
       " (b'hill noises', 279.6550561797753),\n",
       " (b'next morning', 196.21048482459597),\n",
       " (b'twenty four', 191.9359938307307),\n",
       " (b'ex queen', 2154.917748917749)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = []\n",
    "for phrase in bigram.export_phrases(data_words[:100]):\n",
    "    bigrams.append(phrase)\n",
    "bigrams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:23:39.255472Z",
     "start_time": "2019-02-03T09:23:38.041557Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "train_text = remove_stopwords(train['text'])\n",
    "test_text = remove_stopwords(test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:23:42.134679Z",
     "start_time": "2019-02-03T09:23:42.100679Z"
    }
   },
   "outputs": [],
   "source": [
    "train_text = [' '.join(sent) for sent in train_text]\n",
    "test_text = [' '.join(sent) for sent in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:23:44.416338Z",
     "start_time": "2019-02-03T09:23:44.052364Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 6000)\n",
    "\n",
    "feature_vec = vectorizer.fit_transform(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:23:47.002965Z",
     "start_time": "2019-02-03T09:23:46.992965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19579, 6000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(feature_vec)\n",
    "X_train_tf = tf_transformer.transform(feature_vec)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifier Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:23:52.404970Z",
     "start_time": "2019-02-03T09:23:52.402970Z"
    }
   },
   "outputs": [],
   "source": [
    "# create train/test set\n",
    "train_data = train_text\n",
    "train_labels = train[\"author\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(train_data,train_labels,test_size=0.20,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:23:59.388049Z",
     "start_time": "2019-02-03T09:23:59.070470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15663x22935 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 199628 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature processing pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_features = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "])\n",
    "\n",
    "text_features.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:24:02.209663Z",
     "start_time": "2019-02-03T09:24:01.707899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8138406537282942\n",
      "Log loss: 0.596960435695908\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('features', text_features),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "nb_pred = pipe.predict(X_test)\n",
    "nb_probs = pipe.predict_proba(X_test)\n",
    "\n",
    "print(\"Accuracy score: \" + str(accuracy_score(y_test, nb_pred)))\n",
    "print(\"Log loss: \" + str(log_loss(y_test, nb_probs)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:24:13.467404Z",
     "start_time": "2019-02-03T09:24:13.465404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'features', 'clf', 'features__memory', 'features__steps', 'features__vect', 'features__tfidf', 'features__vect__analyzer', 'features__vect__binary', 'features__vect__decode_error', 'features__vect__dtype', 'features__vect__encoding', 'features__vect__input', 'features__vect__lowercase', 'features__vect__max_df', 'features__vect__max_features', 'features__vect__min_df', 'features__vect__ngram_range', 'features__vect__preprocessor', 'features__vect__stop_words', 'features__vect__strip_accents', 'features__vect__token_pattern', 'features__vect__tokenizer', 'features__vect__vocabulary', 'features__tfidf__norm', 'features__tfidf__smooth_idf', 'features__tfidf__sublinear_tf', 'features__tfidf__use_idf', 'clf__alpha', 'clf__class_prior', 'clf__fit_prior'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:24:29.255127Z",
     "start_time": "2019-02-03T09:24:13.471404Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "log_loss_build = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "parameters = {'features__vect__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "              'features__tfidf__use_idf': [False],\n",
    "              'clf__alpha': [0.01]\n",
    "             }\n",
    "\n",
    "gs = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, scoring=log_loss_build)\n",
    " \n",
    "# Fit and tune model\n",
    "gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:24:30.779425Z",
     "start_time": "2019-02-03T09:24:29.255127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01,\n",
       " 'features__tfidf__use_idf': False,\n",
       " 'features__vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-0.4127716645707371"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), pr...,\n",
       "         use_idf=False))])), ('clf', MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8281409601634321\n",
      "Log loss: 0.414406729221749\n"
     ]
    }
   ],
   "source": [
    "gs.best_params_\n",
    "gs.best_score_\n",
    "final_model = gs.best_estimator_\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "final_pred = final_model.predict(X_test)\n",
    "final_probs = final_model.predict_proba(X_test);\n",
    "\n",
    "print(\"Accuracy score: \" + str(accuracy_score(y_test, final_pred)))\n",
    "print(\"Log loss: \" + str(log_loss(y_test, final_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:24:31.447609Z",
     "start_time": "2019-02-03T09:24:30.784425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8220122574055159\n",
      "Log loss: 0.5088275380645537\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf', ComplementNB(alpha=0.01))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "cnb_pred = pipe.predict(X_test)\n",
    "cnb_probs = pipe.predict_proba(X_test);\n",
    "\n",
    "print(\"Accuracy score: \" + str(accuracy_score(y_test, cnb_pred)))\n",
    "print(\"Log loss: \" + str(log_loss(y_test, cnb_probs)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:25:19.595665Z",
     "start_time": "2019-02-03T09:24:47.793456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.7137385086823289\n",
      "Log loss: 0.717271658158377\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf', RandomForestClassifier(n_estimators = 100)) \n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "rf_pred = pipe.predict(X_test)\n",
    "rf_probs = pipe.predict_proba(X_test);\n",
    "\n",
    "print(\"Accuracy score: \" + str(accuracy_score(y_test, rf_pred)))\n",
    "print(\"Log loss: \" + str(log_loss(y_test, rf_probs)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:25:23.918786Z",
     "start_time": "2019-02-03T09:25:23.916786Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'vect', 'tfidf', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'clf__bootstrap', 'clf__class_weight', 'clf__criterion', 'clf__max_depth', 'clf__max_features', 'clf__max_leaf_nodes', 'clf__min_impurity_decrease', 'clf__min_impurity_split', 'clf__min_samples_leaf', 'clf__min_samples_split', 'clf__min_weight_fraction_leaf', 'clf__n_estimators', 'clf__n_jobs', 'clf__oob_score', 'clf__random_state', 'clf__verbose', 'clf__warm_start'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:30:39.391248Z",
     "start_time": "2019-02-03T09:25:58.387445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "log_loss_build = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "parameters = {'clf__n_estimators': [500]}\n",
    "\n",
    "gs = GridSearchCV(pipe, parameters, cv=3, n_jobs=-1, scoring=log_loss_build)\n",
    " \n",
    "# Fit and tune model\n",
    "gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:33:03.912137Z",
     "start_time": "2019-02-03T09:30:39.391248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__n_estimators': 500}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-0.7395881162197039"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.7188457609805925\n",
      "Log loss: 0.706407868807051\n"
     ]
    }
   ],
   "source": [
    "gs.best_params_\n",
    "gs.best_score_\n",
    "final_rf = gs.best_estimator_\n",
    "\n",
    "final_rf.fit(X_train, y_train)\n",
    "rf_pred = final_rf.predict(X_test)\n",
    "rf_probs = final_rf.predict_proba(X_test);\n",
    "\n",
    "print(\"Accuracy score: \" + str(accuracy_score(y_test, rf_pred)))\n",
    "print(\"Log loss: \" + str(log_loss(y_test, rf_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:33:03.927736Z",
     "start_time": "2019-02-03T09:33:03.912137Z"
    }
   },
   "outputs": [],
   "source": [
    "final_rf = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:33:04.442920Z",
     "start_time": "2019-02-03T09:33:03.927736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.7665985699693565\n",
      "Log loss: 0.6797077309738855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf', SGDClassifier(loss='log')) \n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "lr_pred = pipe.predict(X_test)\n",
    "lr_probs = pipe.predict_proba(X_test);\n",
    "\n",
    "print(\"Accuracy score: \" + str(accuracy_score(y_test, lr_pred)))\n",
    "print(\"Log loss: \" + str(log_loss(y_test, lr_probs)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:33:04.443920Z",
     "start_time": "2019-02-03T09:33:04.442920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'vect', 'tfidf', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'clf__alpha', 'clf__average', 'clf__class_weight', 'clf__early_stopping', 'clf__epsilon', 'clf__eta0', 'clf__fit_intercept', 'clf__l1_ratio', 'clf__learning_rate', 'clf__loss', 'clf__max_iter', 'clf__n_iter', 'clf__n_iter_no_change', 'clf__n_jobs', 'clf__penalty', 'clf__power_t', 'clf__random_state', 'clf__shuffle', 'clf__tol', 'clf__validation_fraction', 'clf__verbose', 'clf__warm_start'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:33:55.386347Z",
     "start_time": "2019-02-03T09:33:04.475119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...m_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'clf__alpha': array([1.e-01, 1.e-02, 1.e-03, 1.e-04, 1.e-05, 1.e-06]), 'clf__penalty': ['l1', 'l2'], 'clf__max_iter': [10, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(log_loss, greater_is_better=False, needs_proba=True),\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 1e-05, 'clf__max_iter': 10, 'clf__penalty': 'l2'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search\n",
    "alpha_range = 10.0**-np.arange(1,7)\n",
    "\n",
    "parameters = {'clf__alpha': alpha_range,\n",
    "              'clf__penalty': ['l1', 'l2'],\n",
    "              'clf__max_iter': [10, 50]\n",
    "             }\n",
    "\n",
    "gs = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, scoring=log_loss_build)\n",
    " \n",
    "# Fit and tune model\n",
    "gs.fit(X_train, y_train);\n",
    "\n",
    "gs.best_params_\n",
    "lr_model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:33:55.934329Z",
     "start_time": "2019-02-03T09:33:55.386347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 1e-05, 'clf__max_iter': 10, 'clf__penalty': 'l2'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-0.5126601146097466"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...m_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8087334014300307\n",
      "Log loss: 0.5115457354816121\n"
     ]
    }
   ],
   "source": [
    "gs.best_params_\n",
    "gs.best_score_\n",
    "lr_model = gs.best_estimator_\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "lr_probs = lr_model.predict_proba(X_test);\n",
    "\n",
    "print(\"Accuracy score: \" + str(accuracy_score(y_test, lr_pred)))\n",
    "print(\"Log loss: \" + str(log_loss(y_test, lr_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:34:32.024140Z",
     "start_time": "2019-02-03T09:33:55.934329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.5740551583248212\n",
      "Log loss: 1.095895725011196\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=600)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf', ada) \n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "ada_pred = pipe.predict(X_test)\n",
    "ada_probs = pipe.predict_proba(X_test);\n",
    "\n",
    "print(\"Accuracy score: \" + str(accuracy_score(y_test, ada_pred)))\n",
    "print(\"Log loss: \" + str(log_loss(y_test, ada_probs)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:38:55.078922Z",
     "start_time": "2019-02-03T09:38:51.942014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.4883968248176938\n"
     ]
    }
   ],
   "source": [
    "# hard voting classifier\n",
    "lr_pred = lr_model.predict_proba(X_test)\n",
    "rf_pred = final_rf.predict_proba(X_test)\n",
    "nb_pred = final_model.predict_proba(X_test)\n",
    "vote_pred = (lr_pred + rf_pred + nb_pred) / 3\n",
    "print(\"Log loss: \" + str(log_loss(y_test, vote_pred)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:40:42.116168Z",
     "start_time": "2019-02-03T09:39:37.581888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogLoss: 0.914 +- 0.005\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softprob', n_estimators=100)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf', xgb_clf)\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipe, X_train, y_train, cv=5, n_jobs=-1, scoring='neg_log_loss')\n",
    "print('LogLoss: %.3f +- %.3f' % (-np.mean(scores), 2*np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:47:05.535796Z",
     "start_time": "2019-02-03T09:41:17.330833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'clf__n_estimators': [200], 'clf__max_depth': [5], 'clf__subsample': [0.5, 0.65], 'clf__colsample_bytree': [0.7, 0.95]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_log_loss', verbose=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'clf__colsample_bytree': 0.95,\n",
       " 'clf__max_depth': 5,\n",
       " 'clf__n_estimators': 200,\n",
       " 'clf__subsample': 0.5}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search xgboost\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softprob')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "    ('clf', xgb_clf)\n",
    "])\n",
    "\n",
    "parameters = {'clf__n_estimators': [200],\n",
    "              'clf__max_depth': [5],\n",
    "              'clf__subsample': [0.5, 0.65],\n",
    "              'clf__colsample_bytree': [0.7, 0.95]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, parameters, cv=3, n_jobs=-1, verbose=1, scoring='neg_log_loss', refit=True)\n",
    " \n",
    "# Fit and tune model\n",
    "gs.fit(X_train, y_train);\n",
    "\n",
    "gs.best_params_\n",
    "xgb_model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:50:01.439030Z",
     "start_time": "2019-02-03T09:50:01.437030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7954727927929858"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:51:18.522726Z",
     "start_time": "2019-02-03T09:51:18.195531Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = test_text\n",
    "predictions = final_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:51:18.523726Z",
     "start_time": "2019-02-03T09:51:18.522726Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(data=predictions, columns = final_model.named_steps['clf'].classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:51:18.534726Z",
     "start_time": "2019-02-03T09:51:18.523726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id02310</th>\n",
       "      <td>0.159072</td>\n",
       "      <td>0.021447</td>\n",
       "      <td>0.819480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id24541</th>\n",
       "      <td>0.944975</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.040194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id00134</th>\n",
       "      <td>0.299065</td>\n",
       "      <td>0.697450</td>\n",
       "      <td>0.003485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27757</th>\n",
       "      <td>0.777884</td>\n",
       "      <td>0.221007</td>\n",
       "      <td>0.001108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id04081</th>\n",
       "      <td>0.890424</td>\n",
       "      <td>0.027232</td>\n",
       "      <td>0.082344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              EAP       HPL       MWS\n",
       "id                                   \n",
       "id02310  0.159072  0.021447  0.819480\n",
       "id24541  0.944975  0.014831  0.040194\n",
       "id00134  0.299065  0.697450  0.003485\n",
       "id27757  0.777884  0.221007  0.001108\n",
       "id04081  0.890424  0.027232  0.082344"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating a submission file\n",
    "result = pd.concat([test[['id']], preds], axis=1)\n",
    "result.set_index('id', inplace = True)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T09:51:18.627726Z",
     "start_time": "2019-02-03T09:51:18.541726Z"
    }
   },
   "outputs": [],
   "source": [
    "result.to_csv(\"C:/Users/1394852/Desktop/ml_project/spooky_author/spooky_pred_6.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "39px",
    "width": "298px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
